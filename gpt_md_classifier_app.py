# ğŸ“ Streamlit Markdown Auto Classifier (Final version)
import streamlit as st
from openai import OpenAI
import os
import tempfile
import shutil
import zipfile
from concurrent.futures import ThreadPoolExecutor, as_completed
from uuid import uuid4
from math import ceil

# âœ… Initialize OpenAI client
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# âœ… UI and page setup
st.set_page_config(page_title="ğŸ“ Markdown ìë™ ë³‘í•© ë¶„ë¥˜ê¸°", page_icon="ğŸ“š", layout="wide")
st.title("ğŸ“ ChatGPT ê¸°ë°˜ Markdown ìë™ ë¶„ë¥˜ + ì£¼ì œ ë³‘í•©")
st.markdown("""
Markdown íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ GPTê°€ ìë™ìœ¼ë¡œ ì£¼ì œë¥¼ ë¶„ì„í•˜ê³  ê´€ë ¨ íŒŒì¼ë¼ë¦¬ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.  
ZIP íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìœ¼ë©°, íŒŒì¼ ìˆ˜ê°€ ë§ì•„ë„ ìë™ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.
""")

# âœ… Init session state
if "processed" not in st.session_state:
    st.session_state.processed = False
    st.session_state.zip_path = None
    st.session_state.grouped = {}
    st.session_state.temp_dir = None
    st.session_state.file_infos = []

# âœ… Manual Reset Button
if st.button("ğŸ”„ ë‹¤ì‹œ ì‹œì‘"):
    if st.session_state.temp_dir and os.path.exists(st.session_state.temp_dir):
        shutil.rmtree(st.session_state.temp_dir)
    for key in list(st.session_state.keys()):
        del st.session_state[key]
    st.experimental_rerun()

# âœ… File uploader
uploaded_files = st.file_uploader("â¬†ï¸ Markdown (.md) íŒŒì¼ ì—…ë¡œë“œ", type="md", accept_multiple_files=True)

# âœ… GPT topic + summary extraction
def get_topic_and_summary(filename, content):
    prompt = f"""
ë‹¤ìŒì€ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œì˜ ì£¼ìš” ì£¼ì œë¥¼ ì§§ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ, í•µì‹¬ ìš”ì•½ë„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ì¶œë ¥ í˜•ì‹:
ì£¼ì œ: [ì£¼ì œëª…]
ìš”ì•½: [ìš”ì•½ë‚´ìš©]

ë¬¸ì„œ ì œëª©: {filename}
ë‚´ìš©:
{content[:1000].rsplit('\\n', 1)[0]}...
"""
    try:
        res = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        text = res.choices[0].message.content.strip()
        topic, summary = "Unknown", ""
        for line in text.split("\n"):
            if line.lower().startswith("ì£¼ì œ:"):
                topic = line.split(":", 1)[1].strip()
            elif line.lower().startswith("ìš”ì•½:"):
                summary = line.split(":", 1)[1].strip()
        return topic, summary
    except Exception as e:
        return "Unknown", ""

# âœ… GPT: Grouping
def get_grouped_topics(file_infos):
    merge_prompt = """
ë‹¤ìŒì€ ì—¬ëŸ¬ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì˜ ì£¼ì œ ë° ìš”ì•½ì…ë‹ˆë‹¤. ì£¼ì œì™€ ìš”ì•½ì´ ìœ ì‚¬í•˜ê±°ë‚˜ ê´€ë ¨ ìˆëŠ” íŒŒì¼ë¼ë¦¬ ë¬¶ì–´ 5~10ê°œì˜ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ ì£¼ì„¸ìš”.
ê·¸ë¦¬ê³  ê° ê·¸ë£¹ì— ì ì ˆí•œ ëŒ€í‘œ í‚¤ì›Œë“œë¥¼ 3~5ê°œ, ì‹œë„ˆì§€ê°€ ìˆì„ ë§Œí•œ ë‚´ìš©ì„ ê°™ì´ ìƒì„±í•´ì£¼ì„¸ìš”.
ì¶œë ¥ í˜•ì‹:
[ê·¸ë£¹ëª…]: íŒŒì¼1.md, íŒŒì¼2.md
í‚¤ì›Œë“œ: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3
ìš”ì•½ ë‚´ìš© : ì´ ë‘˜ì€ 

ëª©ë¡:
"""
        for info in chunk:
            prompt += f"- {info['unique_filename']}: {info['topic']} / {info['summary']}\n"

        try:
            res = client.chat.completions.create(
                model="gpt-5-nano",
                messages=[{"role": "user", "content": prompt}]
            )
            text = res.choices[0].message.content.strip()
            current_group = None
            for line in text.split("\n"):
                if ":" in line and ".md" in line:
                    topic, files_str = line.split(":", 1)
                    filenames = [f.strip() for f in files_str.split(",") if f.strip()]
                    current_group = topic.strip() + f" (Batch {i+1})"
                    grouped[current_group] = {"files": filenames, "keywords": []}
                elif "í‚¤ì›Œë“œ:" in line and current_group:
                    keyword_str = line.split(":", 1)[1]
                    grouped[current_group]["keywords"] = [k.strip() for k in keyword_str.split(",")]
        except Exception as e:
            st.warning(f"âš ï¸ ê·¸ë£¹ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    return grouped

# âœ… Process files (only once per session)
if uploaded_files and not st.session_state.processed:
    st.subheader("ğŸ“Š íŒŒì¼ ë¶„ì„ ì¤‘...")
    file_infos = []
    file_id_map = {}
    future_to_file = {}

    with ThreadPoolExecutor(max_workers=5) as executor:
        progress = st.progress(0.0)
        status_text = st.empty()

        for uploaded_file in uploaded_files:
            original_name = uploaded_file.name
            unique_filename = f"{uuid4().hex[:8]}_{original_name}"
            content = uploaded_file.read().decode("utf-8")
            future = executor.submit(get_topic_and_summary, original_name, content)
            future_to_file[future] = {
                "filename": original_name,
                "unique_filename": unique_filename,
                "content": content
            }

        for i, future in enumerate(as_completed(future_to_file)):
            info = future_to_file[future]
            info["topic"], info["summary"] = future.result()
            file_infos.append(info)
            file_id_map[info["unique_filename"]] = info
            progress.progress((i + 1) / len(future_to_file))
            status_text.markdown(f"ğŸ“„ ë¶„ì„ ì¤‘: {i + 1}/{len(future_to_file)}ê°œ ì™„ë£Œ")

    grouped = get_grouped_topics_chunked(file_infos)

    # âœ… Save ZIP
    temp_dir = tempfile.mkdtemp()
    saved_files = []

    for group_name, group_data in grouped.items():
        keywords = group_data.get("keywords", [])
        filenames = group_data["files"]
        folder = os.path.join(temp_dir, group_name.replace(" ", "_").replace("/", "_"))
        os.makedirs(folder, exist_ok=True)

        # README
        readme_path = os.path.join(folder, "README.md")
        with open(readme_path, "w", encoding="utf-8") as readme:
            readme.write(f"# {group_name}\n\n")
            if keywords:
                readme.write(f"**ğŸ“Œ í‚¤ì›Œë“œ:** {', '.join(keywords)}\n\n")
            readme.write("## ğŸ“„ í¬í•¨ëœ íŒŒì¼ ëª©ë¡\n")
            for f in filenames:
                original_name = f.split("_", 1)[-1] if "_" in f else f
                readme.write(f"- {original_name}\n")
            saved_files.append(readme_path)

        for f in filenames:
            match = file_id_map.get(f)
            if match:
                output_path = os.path.join(folder, match["filename"])
                with open(output_path, "w", encoding="utf-8") as out_file:
                    out_file.write(match["content"])
                saved_files.append(output_path)

    # ZIP path
    zip_path = os.path.join(temp_dir, "merged_markdowns.zip")
    with zipfile.ZipFile(zip_path, "w") as zipf:
        for file in saved_files:
            arcname = os.path.relpath(file, temp_dir)
            zipf.write(file, arcname)

    # âœ… Store in session
    st.session_state.processed = True
    st.session_state.zip_path = zip_path
    st.session_state.grouped = grouped
    st.session_state.temp_dir = temp_dir
    st.session_state.file_infos = file_infos

# âœ… Display result if already processed
if st.session_state.processed:
    st.subheader("ğŸ§¾ ë¶„ë¥˜ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
    grouped = st.session_state.grouped
    zip_path = st.session_state.zip_path

    for group_name, group_data in grouped.items():
        st.markdown(f"### ğŸ“ {group_name}")
        st.markdown(f"- ğŸ”‘ í‚¤ì›Œë“œ: {', '.join(group_data.get('keywords', []))}")
        st.markdown(f"- ğŸ“„ íŒŒì¼ ìˆ˜: {len(group_data['files'])}")

    # âœ… Download button (no reprocessing)
    with open(zip_path, "rb") as fp:
        st.download_button("ğŸ“¦ ë³‘í•© ZIP ë‹¤ìš´ë¡œë“œ", fp, file_name="merged_markdowns.zip", mime="application/zip")

    st.caption("â€» ZIP ë‹¤ìš´ë¡œë“œ í›„ì—ë„ ë‹¤ì‹œ ë¶„ì„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œì‘í•˜ë ¤ë©´ ìƒë‹¨ì˜ 'ğŸ”„ ë‹¤ì‹œ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
